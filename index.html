<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xinyu Huang (ÈªÑÊñ∞ÂÆá)</title>
  
  <meta name="author" content="Xinyu Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üê†</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xinyu Huang</name>
              </p>
              <p>I am a second-year Ph.D. student at <a href="https://cs.fudan.edu.cn/"> the School of Computer Science Fudan University</a>, advised by 
                Prof. <a href="https://faculty.fudan.edu.cn/fengrui/zh_CN/index.htm">Rui Feng</a> and Prof. Yuejie Zhang. Meanwhile, I'm also a research intern at OPPO Research Institute, supervised by Researcher <a 
                href="https://scholar.google.com/citations?user=j5idDP8AAAAJ&hl=zh-CN&oi=ao">Youcai Zhang</a>. I was fortunate to work with Prof. <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=zh-CN&oi=ao">Yandong Guo</a> and Prof. <a href="https://www.leizhang.org/">Lei Zhang</a>.
		      

              </p>
              <p>
                My research interests lie in the field of Multimodal Learning and Computer Vision,
                particularly in Vision-Language Pre-training and Multi-label Recognition. My lifelong dream is to create an AI technology like <a href="https://en.wikipedia.org/wiki/J.A.R.V.I.S.">Jarvis</a> that can serve humanity.
              </p>
              <p style="text-align:center">
                <a href="mailto:xinyuhuang20@fudan.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1O5b3VcAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>
                &nbsp/&nbsp
                <a href="https://github.com/xinyu1205">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/xin-yu-77-88-42">Zhihu</a>
              </p>
            <!-- </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xinyuhuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xinyuhuang.jpg" class="hoverZoomLink"></a>
            </td> -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/huangxinyu.jpg"><img style="width:80%;max-width:80" alt="profile photo"
                  src="images/huangxinyu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <heading>Research</heading>(* indicates equal contribution)

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tag2text.png" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://tag2text.github.io/">
                <papertitle> Tag2Text: Guiding Vision-Language Model via Image Tagging </papertitle>
              </a>
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Jinyu Ma,
              Weiwei Tian,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Lei Zhang
              <br>
              <em>Tech report</em>,
              2023
              <br>
              <a href="https://tag2text.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2303.05657">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Tag2Text">code</a>
              <p></p>
              <p>Tag2Text achieves <strong> superior image tag recognition ability </strong> of 3,429 commonly human-used categories. By leveraging tagging guidance, Tag2Text effectively enhances the performance of vision-language models on both <strong> generation-based </strong> and <strong> alignment-based tasks</strong>. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/idea.png" alt="idea" width="190" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2207.05333">
                <papertitle> IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training </papertitle>
              </a>
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Ying Cheng,
              Weiwei Tian,
              Ruiwei Zhao,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Xiaobo Zhang
              <br>
              <em>ACM MM</em>,
              2022
              <br>
              <a href="https://arxiv.org/abs/2207.05333">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/IDEA-pytorch">code</a>
              <!-- <p></p> -->
              <p>
                We propose IDEA to provide more explicit textual supervision (including multiple valuable tags and texts composed by multiple tags) for visual models.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mlml.png" alt="mlml" width="190" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.07368">
                <papertitle>Simple and Robust Loss Design for Multi-Label Learning with Missing Labels</papertitle>
              </a>
              <br>
              Youcai Zhang*,
              Yuhao Cheng*,
              <strong>Xinyu Huang*</strong>,
              Fei Wen,
              Rui Feng,
              Yaqian Li,
              Yandong Guo
              <br>
              <em>Tech report</em>,
              2021
              <br>
              <a href="https://arxiv.org/abs/2112.07368">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/robust-loss-mlml">code</a>
              <!-- <p></p> -->
              <p>
                Multi-label learning in the presence of missing labels(MLML) is a challenging problem. We propose two simple yet effective methods via robust loss design based on an observation. 
              </p>
            </td>
          </tr>


    </tbody></table>


    <table
    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Projects & Resources</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/tag2text_grounded_cam.png" alt="tag2text" width="190" height="110">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">
          <papertitle> Tag2Text & Grounded-SAM </papertitle>
        </a>
        <br>
        project contributed by <strong>Xinyu Huang</strong>
        <br>
        <span style="color: red;">7K+ stars! </span>
        <p></p>
        <p><strong>Tag2Text marry Grounded-SAM</strong>, which can <strong>automatically recognize, detect, and segment</strong> for an image! Tag2Text showcases <strong>powerful image recognition capabilities!</strong> </p>
      </td>
    </tr>




</tbody></table>


			
      </td>
    </tr>
  </table>
</body>

</html>
