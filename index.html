<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xinyu Huang (ÈªÑÊñ∞ÂÆá)</title>
  
  <meta name="author" content="Xinyu Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üê†</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xinyu Huang</name>
              </p>
              <p>I am a third-year Ph.D. student at <a href="https://cs.fudan.edu.cn/"> the School of Computer Science Fudan University</a>, advised by 
                Prof. <a href="https://faculty.fudan.edu.cn/fengrui/zh_CN/index.htm">Rui Feng</a> and Prof. Yuejie Zhang. Meanwhile, I'm also a research intern at OPPO Research Institute, supervised by Researcher <a 
                href="https://scholar.google.com/citations?user=j5idDP8AAAAJ&hl=zh-CN&oi=ao">Youcai Zhang</a>. I was fortunate to work with Prof. <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=zh-CN&oi=ao">Yandong Guo</a> and Prof. <a href="https://www.leizhang.org/">Lei Zhang</a>.
		      

              </p>
              <p>
		My research interests include computer vision and multi-modality. I created the <a href="https://github.com/xinyu1205/recognize-anything" style="color: red;">Recognize Anything Model (RAM) Family</a>, which is a series of open-source and powerful image recognition models.
    
    <!-- lie in the field of Large Models on Visual Recognition and Multi-Modal. -->
<!--                 My research interests lie in the field of Multimodal Learning and Computer Vision, -->
<!--                 particularly in Vision-Language Pre-training and Multi-label Recognition.  -->
                <!-- The <a href="https://recognize-anything.github.io/">Recognize Anything Model (RAM)</a> I created is <span style="color: red;">the strongest open source image recognition model.</span> -->
                <!-- The <a href="https://tag2text.github.io/">Tag2Text Model</a> I created is an <span style="color: red;">efficient and controllable vision-language model</span> with tagging guidance. -->
                
                <!-- My lifelong dream is to create an AI technology like <a href="https://en.wikipedia.org/wiki/J.A.R.V.I.S.">Jarvis</a> that can serve humanity. -->
              </p>
              <p style="text-align:center">
                <a href="mailto:xinyuhuang20@fudan.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1O5b3VcAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>
                &nbsp/&nbsp
                <a href="https://github.com/xinyu1205">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/xin-yu-77-88-42">Zhihu</a>
              </p>
            <!-- </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xinyuhuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/xinyuhuang.jpg" class="hoverZoomLink"></a>
            </td> -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/huangxinyu.jpg"><img style="width:80%;max-width:80" alt="profile photo"
                  src="images/huangxinyu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <!-- <heading>Research</heading>(* indicates equal contribution) -->
                <heading>Research</heading> (* indicates equal contribution)

          </td>
            </tr>
        </tbody></table>
        <style>
          img {
            border-radius: 15px;
          }
        </style>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ram_plus_compare.jpg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.15200">
                <papertitle> Recognize Anything Plus Model (RAM++) </papertitle>
              </a>
              <papertitle> <br>Open-Set Image Tagging with Multi-Grained Text Supervision </papertitle>
              <br>
              <strong>Xinyu Huang</strong>, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, Lei Zhang
              <br>
              <em>Arxiv</em>,
              2023
              <br>
              <!-- <a href="https://recognize-anything.github.io/">project page</a> -->
              
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <!-- <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a> -->
              
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p>RAM++ is the next generation of RAM, which can <strong>recognize any category with high accuracy</strong>, including <strong>both predefined common categories and diverse open-set categories</strong>.    </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/localization_and_recognition.jpg" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://recognize-anything.github.io/">
                <papertitle> Recognize Anything Model (RAM) </papertitle>
              </a>
              <papertitle> <br>Recognize Anything: A Strong Image Tagging Model </papertitle>
              <br>
              Youcai Zhang*,
              <strong>Xinyu Huang*</strong>,
              Jinyu Ma*, Zhaoyang Li*, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, Lei Zhang
              <br>
              <em>Arxiv</em>,
              2023
              <br>
              <a href="https://recognize-anything.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2306.03514">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Recognize_Anything-Tag2Text">code</a>
              <p></p>
              <p>RAM is an image tagging model, which can <strong>recognize any common category with high accuracy</strong>.    </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tag2text.png" alt="tag2text" width="190" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://tag2text.github.io/">
                <papertitle> Tag2Text Vision-Language Model </papertitle>
              </a>
              <papertitle> <br>Tag2Text: Guiding Vision-Language Model via Image Tagging </papertitle>
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Jinyu Ma,
              Weiwei Tian,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Lei Zhang
              <br>
              <em>Arxiv</em>,
              2023
              <br>
              <a href="https://tag2text.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2303.05657">arXiv</a>
              /
              <a href="https://huggingface.co/spaces/xinyu1205/Tag2Text">demo</a>
              /
              <a href="https://github.com/xinyu1205/Tag2Text">code</a>
              <p></p>
              <p>Tag2Text is a vision-language model guided by tagging, which can <strong>support caption and tagging simultaneously</strong>.              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/idea.png" alt="idea" width="190" height="140">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2207.05333"> -->
                <papertitle> IDEA: Increasing Text Diversity via Online Multi-Label Recognition for Vision-Language Pre-training </papertitle>
              <!-- </a> -->
              <br>
              <strong>Xinyu Huang</strong>,
              Youcai Zhang,
              Ying Cheng,
              Weiwei Tian,
              Ruiwei Zhao,
              Rui Feng,
              Yuejie Zhang,
              Yaqian Li,
              Yandong Guo,
              Xiaobo Zhang
              <br>
              <em>ACM MM</em>,
              2022
              <br>
              <a href="https://arxiv.org/abs/2207.05333">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/IDEA-pytorch">code</a>
              <p>
                We propose IDEA to provide more explicit textual supervision (including multiple valuable tags and texts composed by multiple tags) for visual models.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mlml.png" alt="mlml" width="190" height="100">
            </td>
            <td width="75%" valign="middle">
              <!-- <a href="https://arxiv.org/abs/2112.07368"> -->
                <papertitle>Simple and Robust Loss Design for Multi-Label Learning with Missing Labels</papertitle>
              <!-- </a> -->
              <br>
              Youcai Zhang*,
              Yuhao Cheng*,
              <strong>Xinyu Huang*</strong>,
              Fei Wen,
              Rui Feng,
              Yaqian Li,
              Yandong Guo
              <br>
              <em>Arxiv</em>,
              2021
              <br>
              <a href="https://arxiv.org/abs/2112.07368">arXiv</a>
              /
              <a href="https://github.com/xinyu1205/robust-loss-mlml">code</a>
              <p>
                Multi-label learning in the presence of missing labels(MLML) is a challenging problem. We propose two simple yet effective methods via robust loss design based on an observation. 
              </p>
            </td>
          </tr>


    </tbody></table>


    <table
    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Projects & Resources</heading>
        </td>
      </tr>
    </tbody>
  </table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/tagging_results.jpg" alt="tag2text" width="190" height="110">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/xinyu1205/recognize-anything">
          <papertitle> Recognize Anything </papertitle>
        </a>
        <br>
        <strong>project owned by Xinyu Huang</strong>
        <br>
        <span style="color: red;">2K+ stars! </span>
        <p></p>
        <p>We provide <strong>Recognize Anything Model (RAM)</strong> and <strong>Tag2Text model</strong> demonstrating <strong>superior image recognition ability!</strong> </p>
      </td>
    </tr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/tag2text_grounded_cam.png" alt="tag2text" width="190" height="110">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">
          <papertitle> RAM/Tag2Text & Grounded-SAM </papertitle>
        </a>
        <br>
        project contributed by <strong>Xinyu Huang</strong>
        <br>
        <span style="color: red;">10K+ stars! </span>
        <p></p>
        <p><strong>RAM/Tag2Text marry Grounded-SAM</strong>, which can <strong>automatically recognize, detect, and segment</strong> for an image! RAM/Tag2Text showcases <strong>powerful image recognition capabilities!</strong> </p>
      </td>
    </tr>




</tbody></table>


			
      </td>
    </tr>
  </table>
</body>

</html>
